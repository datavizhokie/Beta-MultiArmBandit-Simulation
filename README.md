# Beta-MultiArmBandit-Simulation
 
This code simulates consumer responses to multi-choice situations. Thompson Sampling is used to pull samples from each arm's Beta Distribution (starting with a uniform prior of Beta(1,1)). The max sampled theta indicates the arm to be rewarded. Bayesian updates are then applied to each bandit distribution. The learning model eventually produces a winning arm over the specified event space. This is a purely simulated framework with no input data.


![alt text](https://github.com/datavizhokie/Beta-MultiArmBandit-Simulation/blob/master/Tableau%20view.png)
